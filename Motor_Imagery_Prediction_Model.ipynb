{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df453899-5d46-41b9-a74d-6741e85b8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check that we are using a GPU, if not switch runtimes\n",
    "#   using Runtime > Change Runtime Type > GPU\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d05ecaa8-0765-4ad1-b3ac-af8cae0474c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all remaining packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "import os\n",
    "# !pip install mitdeeplearning\n",
    "import mitdeeplearning as mdl\n",
    "from tqdm import tqdm\n",
    "\n",
    "import livelossplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b4616f5-9cc7-483a-b003-589837452009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "INTERVAL_TIME = 3\n",
    "SAMPLE_TIME = 20\n",
    "INTERVAL_OVERLAP = 0.5\n",
    "TIMESTEP_COUNT = math.ceil((SAMPLE_TIME - INTERVAL_TIME) / (INTERVAL_TIME * INTERVAL_OVERLAP))\n",
    "\n",
    "SAMPLE_RATE = 256\n",
    "\n",
    "FEATURES = 20\n",
    "FEATURE_LIST = [\n",
    "    \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\", \n",
    "    \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\", \n",
    "    \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\", \n",
    "    \"Beta_TP9\", \"Beta_AF7\", \"Beta_AF8\", \"Beta_TP10\", \n",
    "    \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "LEFT_DS_LOCATIONS = [\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Left\\\\museMonitor_2022-11-23--20-20-39_1374846198655688552.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Left\\\\museMonitor_2022-11-23--20-21-16_6403773277566742356.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Left\\\\museMonitor_2022-11-23--20-21-16_6403773277566742356.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Left\\\\museMonitor_2022-11-23--20-22-25_9040452647012648168.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Left\\\\museMonitor_2022-11-23--20-22-25_9040452647012648168.csv\",\n",
    "]\n",
    "MIDDLE_DS_LOCATIONS = [\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Middle\\\\museMonitor_2022-11-23--20-24-31_3241552230625887354.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Middle\\\\museMonitor_2022-11-23--20-25-28_1419560302216719579.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Middle\\\\museMonitor_2022-11-23--20-26-00_5339249621360379372.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Middle\\\\museMonitor_2022-11-23--20-26-31_9151412776081488788.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Middle\\\\museMonitor_2022-11-23--20-27-05_8455441510607239183.csv\",\n",
    "]\n",
    "RIGHT_DS_LOCATIONS = [\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Right\\\\museMonitor_2022-11-23--20-10-51_4392798336885910834.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Right\\\\museMonitor_2022-11-23--20-10-51_4392798336885910834.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Right\\\\museMonitor_2022-11-23--20-14-59_8872411055468760272.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Right\\\\museMonitor_2022-11-23--20-15-43_5478785789981537327.csv\",\n",
    "    \"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Dataset\\\\Right\\\\museMonitor_2022-11-23--20-16-22_6405799957103154682.csv\",\n",
    "]\n",
    "\n",
    "DS_COUNT = len(LEFT_DS_LOCATIONS) + len(MIDDLE_DS_LOCATIONS) + len(RIGHT_DS_LOCATIONS)\n",
    "\n",
    "ZERO = 0.00000001\n",
    "LEFT = 0\n",
    "MIDDLE = 1\n",
    "RIGHT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25c22b47-a7f4-40f9-bf47-6fa60d9a35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing dataset\n",
    "# Calculating possible starting points for the samples to be taken from dataset\n",
    "starting_points = [i * INTERVAL_TIME * INTERVAL_OVERLAP for i in range(TIMESTEP_COUNT)]\n",
    "\n",
    "# all_inputs dimensions [sample][SAMPLE_RATE * INTERVAL_TIME][FEATURES] contains all the inputs with data of all features \n",
    "all_inputs = np.zeros((TIMESTEP_COUNT * DS_COUNT, SAMPLE_RATE * INTERVAL_TIME, FEATURES))\n",
    "\n",
    "# all_outputs 0 -> left 1 -> middle 2 -> right\n",
    "all_outputs = np.zeros((TIMESTEP_COUNT * DS_COUNT, SAMPLE_RATE * INTERVAL_TIME))\n",
    "\n",
    "# Adding all left samples\n",
    "for j, location in enumerate(LEFT_DS_LOCATIONS):\n",
    "  df = pd.read_csv(location)\n",
    "  for i, starting_point in enumerate(starting_points):\n",
    "    all_inputs[j * TIMESTEP_COUNT + i] = df.loc[starting_point * SAMPLE_RATE : starting_point * SAMPLE_RATE + INTERVAL_TIME * SAMPLE_RATE - 1, FEATURE_LIST].to_numpy()\n",
    "    for k in range(SAMPLE_RATE * INTERVAL_TIME):\n",
    "      all_outputs[j * TIMESTEP_COUNT + i][k] = LEFT\n",
    "\n",
    "# Adding all middle samples\n",
    "for location in MIDDLE_DS_LOCATIONS:\n",
    "  df = pd.read_csv(location)\n",
    "  for i, starting_point in enumerate(starting_points):\n",
    "    all_inputs[j * TIMESTEP_COUNT + i] = df.loc[starting_point * SAMPLE_RATE : starting_point * SAMPLE_RATE + INTERVAL_TIME * SAMPLE_RATE - 1, FEATURE_LIST].to_numpy()\n",
    "    for k in range(SAMPLE_RATE * INTERVAL_TIME):\n",
    "      all_outputs[j * TIMESTEP_COUNT + i][k] = MIDDLE\n",
    "\n",
    "# Adding all right samples\n",
    "for location in RIGHT_DS_LOCATIONS:\n",
    "  df = pd.read_csv(location)\n",
    "  for i, starting_point in enumerate(starting_points):\n",
    "    all_inputs[j * TIMESTEP_COUNT + i] = df.loc[starting_point * SAMPLE_RATE : starting_point * SAMPLE_RATE + INTERVAL_TIME * SAMPLE_RATE - 1, FEATURE_LIST].to_numpy()\n",
    "    for k in range(SAMPLE_RATE * INTERVAL_TIME):\n",
    "      all_outputs[j * TIMESTEP_COUNT + i][k] = RIGHT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72c96324-d186-4283-a514-47db9b6df543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get batch of samples\n",
    "\n",
    "def get_batch(all_inputs, all_outputs, batch_size):\n",
    "\n",
    "  # Creates an array of size batch_size with random indicies\n",
    "    idx = np.random.choice(len(all_inputs), batch_size)\n",
    "    # train_batch_size = int(batch_size * 0.7)\n",
    "    train_batch_size = batch_size\n",
    "    # test_batch_size = batch_size - train_batch_size\n",
    "  # Takes the sample at that random index and adds it to the batch\n",
    "    x_batch = np.zeros((train_batch_size, SAMPLE_RATE * INTERVAL_TIME, FEATURES))\n",
    "    y_batch = np.zeros((train_batch_size, SAMPLE_RATE * INTERVAL_TIME))\n",
    "    for i in range(train_batch_size):\n",
    "        # print(i)\n",
    "        x_batch[i] = all_inputs[idx[i]]\n",
    "        y_batch[i] = all_outputs[idx[i]]\n",
    "\n",
    "#     x_train = np.zeros((test_batch_size, SAMPLE_RATE * INTERVAL_TIME, FEATURES))\n",
    "#     y_train = np.zeros((test_batch_size, SAMPLE_RATE * INTERVAL_TIME, NUM_CLASSES))\n",
    "#     # print(\"TRAIN\")\n",
    "#     for i in range(train_batch_size, train_batch_size + test_batch_size):\n",
    "#         # print(i)\n",
    "#         x_train[i-train_batch_size] = all_inputs[idx[i]]\n",
    "#         y_train[i-train_batch_size] = all_outputs[idx[i]]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46d114f2-7696-4539-9f61-16d030d249e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "def build_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.InputLayer(\n",
    "          input_shape=(INTERVAL_TIME * SAMPLE_RATE, FEATURES),\n",
    "      ),\n",
    "      tf.keras.layers.Conv1D(\n",
    "          filters=15,\n",
    "          kernel_size=1,\n",
    "          activation='sigmoid',\n",
    "          input_shape=(INTERVAL_TIME * SAMPLE_RATE, FEATURES),\n",
    "      ),\n",
    "      tf.keras.layers.LSTM(\n",
    "          units=10,\n",
    "          return_sequences=True,\n",
    "          recurrent_initializer='glorot_uniform',\n",
    "          recurrent_activation='sigmoid',\n",
    "      ),\n",
    "      tf.keras.layers.Dense(\n",
    "          units=3,\n",
    "          # activation='softmax'\n",
    "      )\n",
    "  ])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f70cd307-baf3-4037-9aec-7fb1d23bf5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (16, 768, 20)\n",
      "Prediction shape:  (3,)\n",
      "(768,)\n",
      "[1 2 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 2 2 0 1 2 1 2 0 0 0\n",
      " 1 1 0 2 0 0 0 1 0 1 1 2 0 2 2 0 0 1 2 0 0 1 0 1 1 0 0 0 0 1 2 0 0 1 0 0 2\n",
      " 0 0 0 1 0 1 0 1 0 1 0 2 1 1 2 0 2 0 0 2 1 1 0 0 0 0 2 2 0 1 2 0 2 0 2 1 0\n",
      " 0 2 0 2 0 0 1 0 0 0 1 2 0 0 1 0 0 0 2 0 2 0 2 2 0 2 2 0 0 0 1 0 0 0 0 1 0\n",
      " 1 0 2 0 2 0 0 0 2 0 0 1 0 0 0 0 2 2 0 2 0 2 0 2 2 0 2 0 0 2 1 0 0 1 0 2 0\n",
      " 2 0 0 1 1 0 0 0 1 1 1 0 1 1 1 2 0 0 1 2 0 0 0 1 2 1 0 0 0 0 2 0 0 1 2 2 0\n",
      " 1 1 0 0 0 2 0 2 1 0 1 0 0 0 1 0 1 1 0 0 0 2 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 2 1 0 0 0 0 1 2 1 1 2 2 0 0 0 0 2 0 2 0 0 1 0 0 1 1 1 2 2 1 1 1 1 1 0 2\n",
      " 0 0 2 0 0 0 0 0 0 1 0 0 2 2 0 1 0 1 2 0 0 1 0 2 0 0 2 1 0 1 1 2 0 0 0 0 0\n",
      " 2 0 0 1 1 2 2 2 0 2 1 0 0 1 2 2 0 1 1 1 2 0 0 1 1 1 2 0 2 2 2 0 2 2 1 2 2\n",
      " 1 2 0 0 1 0 0 0 2 0 0 0 2 2 1 0 1 1 0 1 0 1 1 2 0 0 0 0 1 1 0 2 2 0 0 1 0\n",
      " 1 0 2 1 0 1 1 1 2 1 2 0 0 1 1 0 1 0 2 1 0 2 0 2 0 0 0 0 0 2 0 1 2 1 1 2 0\n",
      " 0 0 0 0 0 2 2 0 0 0 0 0 1 0 0 1 1 2 2 0 2 0 1 0 2 1 0 1 2 0 0 1 0 1 1 0 1\n",
      " 0 0 0 0 2 0 0 1 2 1 0 1 2 0 1 1 0 2 0 0 1 1 1 1 2 0 0 0 1 2 1 0 0 1 1 1 2\n",
      " 0 1 0 0 2 0 2 1 2 0 0 1 1 1 2 0 1 1 0 1 0 2 2 0 0 0 2 2 1 0 1 1 0 2 2 1 0\n",
      " 2 0 0 2 2 0 1 0 0 0 1 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 2 1 2 0 0 1 0\n",
      " 2 0 1 2 1 1 1 1 2 0 1 0 0 0 1 1 2 0 2 1 2 1 2 2 2 1 0 0 2 0 0 0 0 1 0 0 1\n",
      " 2 0 1 0 1 1 2 1 2 0 0 0 0 1 0 2 0 0 0 0 2 2 2 0 0 1 0 0 2 1 2 0 0 1 0 0 1\n",
      " 0 0 2 0 0 2 0 0 0 0 0 1 1 0 1 2 0 0 1 2 0 0 2 1 0 0 2 2 0 2 2 0 0 1 0 0 1\n",
      " 0 0 2 0 2 0 0 2 0 2 1 1 1 0 0 0 0 1 0 1 2 0 1 1 2 1 1 1 1 0 1 0 0 2 1 0 0\n",
      " 0 1 0 0 0 2 1 0 0 0 0 1 1 2 0 0 0 1 0 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "\n",
    "plot_losses = livelossplot.PlotLossesKeras()\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# x_train, y_train, x_test, y_test = get_batch(all_inputs, all_outputs, BATCH_SIZE)\n",
    "\n",
    "x_train, y_train = get_batch(all_inputs, all_outputs, BATCH_SIZE)\n",
    "\n",
    "pred = model(x_train)\n",
    "\n",
    "print(\"Input shape: \", x_train.shape)\n",
    "print(\"Prediction shape: \", pred[0][0].shape)\n",
    "# print(x_train)\n",
    "# print(pred[0])\n",
    "\n",
    "act_preds = tf.random.categorical(pred[0], num_samples=1)\n",
    "act_preds = tf.squeeze(act_preds, axis=-1).numpy()\n",
    "print(act_preds.shape)\n",
    "print(act_preds)\n",
    "\n",
    "# y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "# y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=BATCH_SIZE,\n",
    "#           epochs=EPOCHS,\n",
    "#           # callbacks=[plot_losses],\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test)\n",
    "#           )\n",
    "  \n",
    "# score = model.evaluate(x_test, y_test, batch_size = BATCH_SIZE, verbose=1)\n",
    "# pred = model.predict(x_train)\n",
    "\n",
    "# print(model.compute_loss(x_test, y_test, pred))\n",
    "\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2747e2a7-b94f-4d80-b61f-bca80daf7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7020404\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    return loss\n",
    "\n",
    "example_batch_loss = compute_loss(y_train, pred)\n",
    "print(example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6aba400-6994-42da-a883-569a0214bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "checkpoint_dir ='./training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32c20ec9-96f0-460f-be11-ffecc419a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab2c2a13-a67b-4e13-8f50-90b0b711e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        \n",
    "        loss = compute_loss(y, y_hat)\n",
    "        \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6856a717-dc3c-4b5b-ba81-85fcf5024fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3656483]\n",
      "[1.3656483, 1.1549546]\n",
      "[1.3656483, 1.1549546, 1.035693]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.3656483, 1.1549546, 1.035693, 0.92135745, 0.79688245, 0.7607452, 0.7121947, 0.6068655, 0.7067382, 0.44140783, 0.5812756, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "NUM_TRAINING_ITERATIONS = 100\n",
    "# plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "\n",
    "# if hasattr(tqdm, '_instances'): tqdm._instances.clear()\n",
    "\n",
    "for iter in range(NUM_TRAINING_ITERATIONS):\n",
    "    x_batch, y_batch = get_batch(all_inputs, all_outputs, BATCH_SIZE)\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "    \n",
    "    history.append(loss.numpy().mean())\n",
    "    print(history)\n",
    "    # plotter.plot(history)\n",
    "    \n",
    "    if iter % 100 == 0:\n",
    "        model.save_weights(checkpoint_prefix)\n",
    "        \n",
    "model.save_weights(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "445e122a-ce6d-489e-9138-4b900df84e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model, to_file='model.png', show_shapes=False, show_layer_names=True,\n",
    "    rankdir='TB', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76292f89-2c77-4379-9054-6e5e63c5448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Chanakya\\BDR 2022\\Saved_Models\\Model_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Chanakya\\BDR 2022\\Saved_Models\\Model_1\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "# Change model number\n",
    "model.save(\"C:\\\\Users\\\\Chanakya\\\\BDR 2022\\\\Saved_Models\\\\Model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3d4a0-f945-41ab-bdda-a2e3240025d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9f2e0-3131-4fe4-a83e-c73cabd0e12b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
